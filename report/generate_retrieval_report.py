#!/usr/bin/env python3
"""Generate `report/retrieval.md` from experiment results.

This script reads the latest result artifacts under `experiments/retrieval/results/`
and produces a Markdown report that matches the figures generated by
`visualization/generate_all.py`.
"""

from __future__ import annotations

import json
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple


PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


@dataclass(frozen=True)
class ResultPaths:
    baseline: Optional[Path]
    pretrained: Optional[Path]
    deep: Optional[Path]
    grid_search_dir: Optional[Path]
    ablations_dir: Optional[Path]
    robustness_dir: Optional[Path]
    efficiency_dir: Optional[Path]
    fusion_dir: Optional[Path]
    twostage_dir: Optional[Path]
    partial_dir: Optional[Path]


def _latest_timestamp_dir(base: Path) -> Optional[Path]:
    if not base.exists():
        return None
    dirs = [d for d in base.iterdir() if d.is_dir() and d.name.startswith('20')]
    if not dirs:
        return None
    return sorted(dirs)[-1]


def _load_json(path: Path) -> Any:
    with open(path, 'r') as f:
        return json.load(f)


def _pct(x: float, decimals: int = 2) -> str:
    return f"{x * 100:.{decimals}f}"


def _fmt_float(x: float, decimals: int = 3) -> str:
    return f"{x:.{decimals}f}"


def _safe_get(d: Dict[str, Any], key: str, default: float = 0.0) -> float:
    v = d.get(key, default)
    try:
        return float(v)
    except Exception:
        return float(default)


def _ci_half_width_from_ci(ci_entry: Optional[Dict[str, Any]]) -> float:
    if not ci_entry:
        return 0.0
    if 'width' in ci_entry and ci_entry['width'] is not None:
        return float(ci_entry['width']) / 2.0
    if 'lower' in ci_entry and 'upper' in ci_entry and ci_entry['lower'] is not None and ci_entry['upper'] is not None:
        return (float(ci_entry['upper']) - float(ci_entry['lower'])) / 2.0
    return 0.0


def _ci_half_width_from_std(std_value: float, num_folds: int = 5, z: float = 1.96) -> float:
    # Normal approximation of CI half-width for the mean.
    if num_folds <= 0:
        return 0.0
    return float(z) * float(std_value) / (num_folds ** 0.5)


def _md_table(headers: List[str], rows: List[List[str]]) -> str:
    header_line = "| " + " | ".join(headers) + " |"
    sep_line = "|" + "|".join(["---"] * len(headers)) + "|"
    body = "\n".join(["| " + " | ".join(r) + " |" for r in rows])
    return "\n".join([header_line, sep_line, body])


def _paths() -> ResultPaths:
    results_dir = PROJECT_ROOT / "experiments" / "retrieval" / "results"

    baseline_dir = _latest_timestamp_dir(results_dir)
    pretrained_dir = _latest_timestamp_dir(results_dir / "pretrained")
    deep_dir = _latest_timestamp_dir(results_dir / "deep_retrievers")

    return ResultPaths(
        baseline=(baseline_dir / "results.json") if baseline_dir else None,
        pretrained=(pretrained_dir / "results.json") if pretrained_dir else None,
        deep=(deep_dir / "results.json") if deep_dir else None,
        grid_search_dir=_latest_timestamp_dir(results_dir / "grid_search"),
        ablations_dir=_latest_timestamp_dir(results_dir / "ablations"),
        robustness_dir=_latest_timestamp_dir(results_dir / "robustness"),
        efficiency_dir=_latest_timestamp_dir(results_dir / "efficiency"),
        fusion_dir=_latest_timestamp_dir(results_dir / "fusion"),
        twostage_dir=_latest_timestamp_dir(results_dir / "twostage"),
        partial_dir=_latest_timestamp_dir(results_dir / "partial"),
    )


def _load_method_results(paths: ResultPaths) -> Dict[str, Dict[str, Any]]:
    all_methods: Dict[str, Dict[str, Any]] = {}

    def add_category(results: Dict[str, Any], category: str):
        for method, data in results.get('methods', {}).items():
            all_methods[method] = {
                'category': category,
                'mean': data.get('mean', {}),
                'std': data.get('std', {}),
                'ci': data.get('ci', {}),
                'folds': data.get('folds', {}),
            }

    if paths.baseline and paths.baseline.exists():
        add_category(_load_json(paths.baseline), 'traditional')
    if paths.deep and paths.deep.exists():
        add_category(_load_json(paths.deep), 'deep')
    if paths.pretrained and paths.pretrained.exists():
        add_category(_load_json(paths.pretrained), 'pretrained')

    return all_methods


def _method_comparison_section(all_methods: Dict[str, Dict[str, Any]]) -> str:
    # Lazy import to keep report generation independent of plotting backends.
    from visualization.config import get_method_name

    cat_order = {'traditional': 0, 'deep': 1, 'pretrained': 2}
    methods_sorted = sorted(
        all_methods.items(),
        key=lambda kv: (cat_order.get(kv[1].get('category', 'traditional'), 99), -_safe_get(kv[1].get('mean', {}), 'hit@10')),
    )

    headers = [
        'Method',
        'Category',
        'Hit@1 (%)',
        'Hit@5 (%)',
        'Hit@10 (%)',
        'Hit@20 (%)',
        'P@10 (%)',
        'MRR@10',
        'MAP@10',
        'NDCG@10',
        'Hit@10 CI (±pp)',
    ]
    rows: List[List[str]] = []

    for method, data in methods_sorted:
        mean = data.get('mean', {})
        ci = data.get('ci', {})
        std = data.get('std', {})

        ci_half = _ci_half_width_from_ci(ci.get('hit@10'))
        if ci_half == 0.0:
            ci_half = _ci_half_width_from_std(_safe_get(std, 'hit@10'))

        rows.append([
            f"`{method}` ({get_method_name(method)})" if get_method_name(method) != method else f"`{method}`",
            data.get('category', 'unknown'),
            _pct(_safe_get(mean, 'hit@1')),
            _pct(_safe_get(mean, 'hit@5')),
            _pct(_safe_get(mean, 'hit@10')),
            _pct(_safe_get(mean, 'hit@20')),
            _pct(_safe_get(mean, 'precision@10')),
            _fmt_float(_safe_get(mean, 'mrr@10')),
            _fmt_float(_safe_get(mean, 'map@10')),
            _fmt_float(_safe_get(mean, 'ndcg@10')),
            f"{ci_half * 100:.2f}",
        ])

    text = []
    text.append("## 3. Overall Performance")
    text.append("")
    text.append("![Method Comparison](../visualization/outputs/fig1_method_comparison.png)")
    text.append("")
    text.append(_md_table(headers, rows))
    text.append("")
    text.append(
        "_Hit@10 CI (±pp) uses the stored 95% bootstrap CI when available; "
        "otherwise it is approximated as 1.96·std/√5 from fold-to-fold std._"
    )
    return "\n".join(text)


def _category_summary_section(all_methods: Dict[str, Dict[str, Any]]) -> str:
    from visualization.config import get_method_name

    categories = {'traditional': [], 'deep': [], 'pretrained': []}
    for method, data in all_methods.items():
        categories.get(data.get('category', 'traditional'), []).append((method, data))

    best_rows: List[List[str]] = []
    headers = ['Category', 'Best (Hit@10)', 'Hit@1 (%)', 'Hit@10 (%)', 'Hit@10 CI (±pp)']

    for cat, items in categories.items():
        if not items:
            continue
        items.sort(key=lambda kv: _safe_get(kv[1].get('mean', {}), 'hit@10'), reverse=True)
        method, data = items[0]
        mean = data.get('mean', {})
        ci = data.get('ci', {})
        std = data.get('std', {})
        ci_half = _ci_half_width_from_ci(ci.get('hit@10'))
        if ci_half == 0.0:
            ci_half = _ci_half_width_from_std(_safe_get(std, 'hit@10'))

        best_rows.append([
            cat,
            f"`{method}` ({get_method_name(method)})" if get_method_name(method) != method else f"`{method}`",
            _pct(_safe_get(mean, 'hit@1')),
            _pct(_safe_get(mean, 'hit@10')),
            f"{ci_half * 100:.2f}",
        ])

    text = []
    text.append("## 4. Category Summary")
    text.append("")
    text.append("![Method Categories](../visualization/outputs/fig2_method_categories.png)")
    text.append("")
    text.append(_md_table(headers, best_rows))
    return "\n".join(text)


def _grid_search_section(paths: ResultPaths) -> str:
    if not paths.grid_search_dir:
        return "## 5. Grid Search\n\n_No grid search results found._"

    text = []
    text.append("## 5. Hyperparameter Grid Search")
    text.append("")
    text.append("![Grid Search](../visualization/outputs/fig3_grid_search.png)")
    text.append("")

    # =========================================================================
    # Step 1: Frame/Hop Length Grid Search
    # =========================================================================
    step1 = paths.grid_search_dir / "step1_frame_hop.json"
    if step1.exists():
        data = _load_json(step1)
        configs = data.get('configs', [])
        metrics = data.get('metrics', [])

        combined = []
        for i, cfg in enumerate(configs):
            if i >= len(metrics):
                continue
            m = metrics[i]
            combined.append((
                float(cfg.get('frame_length_ms', 0)),
                float(cfg.get('hop_length_ms', 0)),
                int(cfg.get('n_fft', 0)),
                int(cfg.get('hop_length', 0)),
                _safe_get(m, 'hit@10'),
            ))
        combined.sort(key=lambda x: x[-1], reverse=True)

        headers = ['Rank', 'Frame (ms)', 'Hop (ms)', 'n_fft', 'hop_length', 'Hit@10 (%)']
        rows: List[List[str]] = []
        for idx, (frame_ms, hop_ms, n_fft, hop_len, hit10) in enumerate(combined[:10], start=1):
            rows.append([
                str(idx),
                f"{frame_ms:.0f}",
                f"{hop_ms:.0f}",
                str(n_fft),
                str(hop_len),
                _pct(hit10, decimals=2),
            ])

        text.append("### 5.1 Frame/Hop Length Search")
        text.append("")
        text.append(_md_table(headers, rows))
        text.append("")
    else:
        text.append("### 5.1 Frame/Hop Length Search")
        text.append("")
        text.append("_No frame/hop grid search results found._")
        text.append("")

    # =========================================================================
    # Step 2: MFCC Parameters Grid Search
    # =========================================================================
    step2 = paths.grid_search_dir / "step2_mfcc_params.json"
    if step2.exists():
        data = _load_json(step2)
        configs = data.get('configs', [])
        metrics = data.get('metrics', [])

        combined = []
        for i, cfg in enumerate(configs):
            if i >= len(metrics):
                continue
            m = metrics[i]
            combined.append((
                int(cfg.get('n_mels', 64)),
                int(cfg.get('n_mfcc', 20)),
                float(cfg.get('frame_length_ms', 0)),
                float(cfg.get('hop_length_ms', 0)),
                _safe_get(m, 'hit@10'),
                _safe_get(m, 'hit@1'),
            ))
        combined.sort(key=lambda x: x[4], reverse=True)

        headers = ['Rank', 'n_mels', 'n_mfcc', 'Frame (ms)', 'Hop (ms)', 'Hit@1 (%)', 'Hit@10 (%)']
        rows: List[List[str]] = []
        for idx, (n_mels, n_mfcc, frame_ms, hop_ms, hit10, hit1) in enumerate(combined[:10], start=1):
            rows.append([
                str(idx),
                str(n_mels),
                str(n_mfcc),
                f"{frame_ms:.0f}",
                f"{hop_ms:.0f}",
                _pct(hit1, decimals=2),
                _pct(hit10, decimals=2),
            ])

        text.append("### 5.2 MFCC Parameters Search")
        text.append("")
        text.append(_md_table(headers, rows))
        text.append("")
    else:
        text.append("### 5.2 MFCC Parameters Search")
        text.append("")
        text.append("_No MFCC params grid search results found._")
        text.append("")

    # =========================================================================
    # Step 3: Window Function Comparison
    # =========================================================================
    step3 = paths.grid_search_dir / "step3_window.json"
    if step3.exists():
        data = _load_json(step3)
        configs = data.get('configs', [])
        metrics = data.get('metrics', [])
        fold_results = data.get('fold_results', {})

        combined = []
        for i, cfg in enumerate(configs):
            if i >= len(metrics):
                continue
            m = metrics[i]
            window = cfg.get('window', 'unknown')
            hit10 = _safe_get(m, 'hit@10')
            hit1 = _safe_get(m, 'hit@1')
            mrr = _safe_get(m, 'mrr@10')

            # Calculate std from fold results if available
            std_hit10 = 0.0
            if window in fold_results:
                fold_vals = [_safe_get(fold_results[window][f], 'hit@10') for f in fold_results[window]]
                if fold_vals:
                    import statistics
                    std_hit10 = statistics.stdev(fold_vals) if len(fold_vals) > 1 else 0.0

            combined.append((window, hit1, hit10, mrr, std_hit10))

        combined.sort(key=lambda x: x[2], reverse=True)

        headers = ['Window', 'Hit@1 (%)', 'Hit@10 (%)', 'Std (%)', 'MRR@10']
        rows: List[List[str]] = []
        for window, hit1, hit10, mrr, std_hit10 in combined:
            rows.append([
                window.capitalize(),
                _pct(hit1, decimals=2),
                _pct(hit10, decimals=2),
                f"{std_hit10 * 100:.2f}",
                _fmt_float(mrr),
            ])

        text.append("### 5.3 Window Function Comparison")
        text.append("")
        text.append(_md_table(headers, rows))
        text.append("")
    else:
        text.append("### 5.3 Window Function Comparison")
        text.append("")
        text.append("_No window function comparison results found._")
        text.append("")

    return "\n".join(text).rstrip()


def _ablations_section(paths: ResultPaths) -> str:
    if not paths.ablations_dir:
        return "## 6. Ablations\n\n_No ablation results found._"

    all_path = paths.ablations_dir / "all_ablations.json"
    if not all_path.exists():
        return "## 6. Ablations\n\n_No ablation results found._"

    ablations = _load_json(all_path)

    def summarize(group_key: str, baseline_pred) -> Tuple[str, str]:
        items = ablations.get(group_key, [])
        if not items:
            return "", ""
        baseline = None
        for it in items:
            cfg = it.get('config', {})
            if baseline_pred(cfg):
                baseline = it
                break
        if baseline is None:
            baseline = items[0]

        base_h1 = _safe_get(baseline.get('mean', {}), 'hit@1')
        base_h10 = _safe_get(baseline.get('mean', {}), 'hit@10')

        headers = ['Config', 'Hit@1 (%)', 'Hit@10 (%)', 'Δ Hit@1 (pp)', 'Δ Hit@10 (pp)']
        rows = []
        for it in items:
            cfg = it.get('config', {})
            name = cfg.get('name', cfg.get('cmvn_mode', ''))
            h1 = _safe_get(it.get('mean', {}), 'hit@1')
            h10 = _safe_get(it.get('mean', {}), 'hit@10')
            rows.append([
                str(name),
                _pct(h1),
                _pct(h10),
                f"{(h1 - base_h1) * 100:+.2f}",
                f"{(h10 - base_h10) * 100:+.2f}",
            ])
        return _md_table(headers, rows), f"Baseline: `{baseline.get('config', {}).get('name', 'baseline')}`"

    preemph_table, _ = summarize('preemphasis', lambda c: str(c.get('name', '')).startswith('no'))
    cmvn_table, _ = summarize('cmvn', lambda c: c.get('cmvn_mode', '') in ('none', ''))
    mel_table, _ = summarize('mel_formula', lambda c: c.get('htk', False) is False)

    text = []
    text.append("## 6. Ablation Studies")
    text.append("")
    text.append("![Ablations](../visualization/outputs/fig4_ablations.png)")
    text.append("")
    if preemph_table:
        text.append("### Pre-emphasis")
        text.append(preemph_table)
        text.append("")
    if cmvn_table:
        text.append("### CMVN")
        text.append(cmvn_table)
        text.append("")
    if mel_table:
        text.append("### Mel Formula")
        text.append(mel_table)
        text.append("")
    return "\n".join(text).rstrip()


def _robustness_section(paths: ResultPaths) -> str:
    if not paths.robustness_dir:
        return "## 7. Robustness\n\n_No robustness results found._"

    all_path = paths.robustness_dir / "all_robustness.json"
    if not all_path.exists():
        return "## 7. Robustness\n\n_No robustness results found._"

    data = _load_json(all_path)

    clean = data.get('clean', {})
    clean_h1 = _safe_get(clean.get('mean', {}), 'hit@1')

    def row_for(key: str) -> Optional[List[str]]:
        if key not in data:
            return None
        it = data[key]
        h1 = _safe_get(it.get('mean', {}), 'hit@1')
        h10 = _safe_get(it.get('mean', {}), 'hit@10')
        rel = (h1 - clean_h1) / clean_h1 * 100 if clean_h1 > 0 else 0.0
        return [key, _pct(h1), _pct(h10), f"{rel:+.1f}%"]

    order = [
        'clean',
        'noise_20dB', 'noise_10dB', 'noise_0dB',
        'volume_+6dB', 'volume_-6dB',
        'speed_0.9x', 'speed_1.1x',
        'pitch_-1', 'pitch_+1',
        'time_shift_0.1', 'time_shift_0.2',
    ]
    rows = [r for r in (row_for(k) for k in order) if r is not None]

    text = []
    text.append("## 7. Robustness")
    text.append("")
    text.append("![Robustness](../visualization/outputs/fig5_robustness.png)")
    text.append("")
    text.append(_md_table(['Condition', 'Hit@1 (%)', 'Hit@10 (%)', 'Δ Hit@1 vs Clean'], rows))
    return "\n".join(text)


def _efficiency_section(paths: ResultPaths, all_methods: Dict[str, Dict[str, Any]]) -> str:
    if not paths.efficiency_dir:
        return "## 8. Efficiency\n\n_No efficiency results found._"

    timing_path = paths.efficiency_dir / "timing.json"
    if not timing_path.exists():
        return "## 8. Efficiency\n\n_No efficiency results found._"

    timing = _load_json(timing_path)
    methods = timing.get('methods', [])

    headers = ['Method', 'Feature Extract (ms)', 'Retrieval (ms)', 'QPS', 'Memory (MB)', 'Dim', 'Hit@10 (%)']
    rows: List[List[str]] = []
    for it in methods:
        method = it.get('method_name', '')
        hit10 = _safe_get(all_methods.get(method, {}).get('mean', {}), 'hit@10', default=0.0)
        rows.append([
            f"`{method}`",
            f"{float(it.get('feature_extract_time_ms', 0)):.2f} ± {float(it.get('feature_extract_std_ms', 0)):.2f}",
            f"{float(it.get('retrieval_time_ms', 0)):.2f} ± {float(it.get('retrieval_std_ms', 0)):.2f}",
            f"{float(it.get('throughput_qps', 0)):.1f}",
            f"{float(it.get('gallery_memory_mb', 0)):.2f}",
            str(int(it.get('feature_dim', 0))),
            _pct(hit10),
        ])

    text = []
    text.append("## 8. Efficiency")
    text.append("")
    text.append("![Efficiency](../visualization/outputs/fig6_efficiency.png)")
    text.append("")
    text.append(_md_table(headers, rows))
    text.append("")
    text.append("_Note: Efficiency benchmarking is currently available for the traditional (M1–M7) methods measured in `timing.json`._")
    return "\n".join(text).rstrip()


def _fusion_twostage_section(paths: ResultPaths) -> str:
    text = []
    text.append("## 9. Fusion & Two-Stage Retrieval")
    text.append("")
    text.append("![Fusion and Two-Stage](../visualization/outputs/fig7_fusion_twostage.png)")
    text.append("")

    if paths.fusion_dir:
        fusion_path = paths.fusion_dir / "all_fusion.json"
    else:
        fusion_path = None

    if fusion_path and fusion_path.exists():
        fusion = _load_json(fusion_path)

        # Best individual
        indiv = fusion.get('individual_methods', {})
        best_ind_name = None
        best_ind = None
        best_h10 = -1.0
        for name, d in indiv.items():
            h10 = _safe_get(d.get('mean', {}), 'hit@10')
            if h10 > best_h10:
                best_h10 = h10
                best_ind_name = name
                best_ind = d

        late_best = fusion.get('late_fusion', {}).get('best', {})
        rrf = fusion.get('rank_fusion', {})

        rows = []
        if best_ind_name and best_ind:
            rows.append([
                f"Best Individual ({best_ind_name})",
                _pct(_safe_get(best_ind.get('mean', {}), 'hit@1')),
                _pct(_safe_get(best_ind.get('mean', {}), 'hit@10')),
                _fmt_float(_safe_get(best_ind.get('mean', {}), 'mrr@10')),
            ])
        if late_best:
            w = late_best.get('weights', {})
            w_str = ", ".join([f"{k}:{v:.2f}" for k, v in w.items()]) if w else "best"
            rows.append([
                f"Late Fusion ({w_str})",
                _pct(_safe_get(late_best.get('mean', {}), 'hit@1')),
                _pct(_safe_get(late_best.get('mean', {}), 'hit@10')),
                _fmt_float(_safe_get(late_best.get('mean', {}), 'mrr@10')),
            ])
        if rrf:
            rows.append([
                "Rank Fusion (RRF)",
                _pct(_safe_get(rrf.get('mean', {}), 'hit@1')),
                _pct(_safe_get(rrf.get('mean', {}), 'hit@10')),
                _fmt_float(_safe_get(rrf.get('mean', {}), 'mrr@10')),
            ])
        if rows:
            text.append(_md_table(['Method', 'Hit@1 (%)', 'Hit@10 (%)', 'MRR@10'], rows))
            text.append("")
    else:
        text.append("_No fusion results found._")
        text.append("")

    # Two-stage summary table
    if paths.twostage_dir:
        twostage_path = paths.twostage_dir / "n_sweep.json"
    else:
        twostage_path = None

    if twostage_path and twostage_path.exists():
        sweep = _load_json(twostage_path)
        m5 = sweep.get('M5_baseline', {})
        m5_time = _safe_get(m5.get('mean', {}), 'avg_query_time_ms')
        m5_h10 = _safe_get(m5.get('mean', {}), 'hit@10')

        items = []
        for k, v in sweep.items():
            if k.startswith('TwoStage_N'):
                n = int(v.get('n', k.replace('TwoStage_N', '')))
                h10 = _safe_get(v.get('mean', {}), 'hit@10')
                t = _safe_get(v.get('mean', {}), 'avg_query_time_ms')
                speedup = m5_time / t if t > 0 else 0.0
                retention = h10 / m5_h10 * 100 if m5_h10 > 0 else 0.0
                items.append((n, h10, t, speedup, retention))
        items.sort(key=lambda x: x[0])

        rows = []
        for n, h10, t, sp, ret in items:
            rows.append([
                str(n),
                _pct(h10),
                f"{t:.2f}",
                f"{sp:.2f}×",
                f"{ret:.1f}%",
            ])
        text.append("### Two-Stage N Sweep")
        text.append(_md_table(['N', 'Hit@10 (%)', 'Query Time (ms)', 'Speedup vs M5', 'Accuracy Retention'], rows))
        text.append("")
    else:
        text.append("_No two-stage results found._")
        text.append("")

    text.append("![Two-Stage Pareto](../visualization/outputs/fig9_twostage_pareto.png)")
    return "\n".join(text).rstrip()


def _partial_query_section(paths: ResultPaths) -> str:
    if not paths.partial_dir:
        return "## 10. Partial Query\n\n_No partial query results found._"

    partial_path = paths.partial_dir / "partial_query.json"
    if not partial_path.exists():
        return "## 10. Partial Query\n\n_No partial query results found._"

    data = _load_json(partial_path)
    items = []
    for k, v in data.items():
        dur = float(v.get('duration_s', k.replace('s', '')))
        items.append((dur, v))
    items.sort(key=lambda x: x[0])

    rows = []
    for dur, v in items:
        mean = v.get('mean', {})
        rows.append([
            f"{dur:.1f}",
            _pct(_safe_get(mean, 'hit@1')),
            _pct(_safe_get(mean, 'hit@5')),
            _pct(_safe_get(mean, 'hit@10')),
            _fmt_float(_safe_get(mean, 'mrr@10')),
        ])

    text = []
    text.append("## 10. Partial Query")
    text.append("")
    text.append("![Partial Query](../visualization/outputs/fig8_partial_query.png)")
    text.append("")
    text.append(_md_table(['Duration (s)', 'Hit@1 (%)', 'Hit@5 (%)', 'Hit@10 (%)', 'MRR@10'], rows))
    return "\n".join(text)


def _fold_variance_section(all_methods: Dict[str, Dict[str, Any]]) -> str:
    from visualization.config import get_method_name

    rows = []
    for method, data in sorted(all_methods.items(), key=lambda kv: -_safe_get(kv[1].get('mean', {}), 'hit@10')):
        folds = data.get('folds', {})
        if not folds:
            continue
        fold_vals = [float(folds[f].get('hit@10', 0)) for f in sorted(folds.keys())]
        if not fold_vals:
            continue
        mean_h10 = _safe_get(data.get('mean', {}), 'hit@10')
        std_h10 = _safe_get(data.get('std', {}), 'hit@10')
        cv = (std_h10 / mean_h10 * 100) if mean_h10 > 0 else 0.0
        rows.append([
            f"`{method}` ({get_method_name(method)})" if get_method_name(method) != method else f"`{method}`",
            f"{mean_h10 * 100:.2f} ± {std_h10 * 100:.2f}",
            f"{cv:.1f}",
            f"{min(fold_vals) * 100:.2f}",
            f"{max(fold_vals) * 100:.2f}",
        ])

    text = []
    text.append("## 11. Fold Variance")
    text.append("")
    text.append("![Fold Variance](../visualization/outputs/fig10_fold_variance.png)")
    text.append("")
    if rows:
        text.append(_md_table(['Method', 'Hit@10 Mean ± Std (pp)', 'CV (%)', 'Min Fold', 'Max Fold'], rows))
    else:
        text.append("_No per-fold results found._")
    return "\n".join(text)


def generate() -> str:
    paths = _paths()
    all_methods = _load_method_results(paths)

    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    def p(path: Optional[Path]) -> str:
        if not path:
            return "(missing)"
        try:
            return str(path.relative_to(PROJECT_ROOT))
        except Exception:
            return str(path)

    lines: List[str] = []
    lines.append("# Audio Retrieval Report (ESC-50)")
    lines.append("")
    lines.append(f"Generated: {now}")
    lines.append("")
    lines.append("## 1. Data Sources")
    lines.append("")
    lines.append(f"- Baseline: `{p(paths.baseline)}`")
    lines.append(f"- Deep retrievers: `{p(paths.deep)}`")
    lines.append(f"- Pretrained: `{p(paths.pretrained)}`")
    lines.append(f"- Grid search: `{p((paths.grid_search_dir / 'step1_frame_hop.json') if paths.grid_search_dir else None)}`")
    lines.append(f"- Ablations: `{p((paths.ablations_dir / 'all_ablations.json') if paths.ablations_dir else None)}`")
    lines.append(f"- Robustness: `{p((paths.robustness_dir / 'all_robustness.json') if paths.robustness_dir else None)}`")
    lines.append(f"- Efficiency: `{p((paths.efficiency_dir / 'timing.json') if paths.efficiency_dir else None)}`")
    lines.append(f"- Fusion: `{p((paths.fusion_dir / 'all_fusion.json') if paths.fusion_dir else None)}`")
    lines.append(f"- Two-stage: `{p((paths.twostage_dir / 'n_sweep.json') if paths.twostage_dir else None)}`")
    lines.append(f"- Partial query: `{p((paths.partial_dir / 'partial_query.json') if paths.partial_dir else None)}`")
    lines.append("")

    lines.append("## 2. Figures")
    lines.append("")
    lines.append("Figures are generated under `visualization/outputs/` via `python3 visualization/generate_all.py`.")
    lines.append("")

    lines.append(_method_comparison_section(all_methods))
    lines.append("")
    lines.append(_category_summary_section(all_methods))
    lines.append("")
    lines.append(_grid_search_section(paths))
    lines.append("")
    lines.append(_ablations_section(paths))
    lines.append("")
    lines.append(_robustness_section(paths))
    lines.append("")
    lines.append(_efficiency_section(paths, all_methods))
    lines.append("")
    lines.append(_fusion_twostage_section(paths))
    lines.append("")
    lines.append(_partial_query_section(paths))
    lines.append("")
    lines.append(_fold_variance_section(all_methods))
    lines.append("")

    return "\n".join(lines).replace("\n\n\n", "\n\n")


def main() -> None:
    report_md = generate()
    out_path = PROJECT_ROOT / "report" / "retrieval.md"
    out_path.write_text(report_md, encoding='utf-8')
    print(f"Wrote: {out_path}")


if __name__ == '__main__':
    main()
